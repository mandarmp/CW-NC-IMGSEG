{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_24_Nov.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2guthgOz-Ssm"
      },
      "source": [
        "# Coursework for Cardiac MR Image Segmentation (2021-2022)\n",
        "\n",
        "After you have gone through the coursework description, this tutorial is designed to further helps you understand the problem and therefore enable you to propose a good solution for this coursework. You will learn:\n",
        "\n",
        "* how to load and save images with OpenCV\n",
        "* how to train a segmentation model with Pytorch\n",
        "* how to evaluate the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsnVbP35-Sso"
      },
      "source": [
        "## 1. Load, show, and save images with OpenCV\n",
        "\n",
        "OpenCV is an open-source computer vision library which helps us to manipulate image data. In this section, we will cover:\n",
        "* Loading an image from file with imread()\n",
        "* Displaying the image with matplotlib plt.imshow()\n",
        "* Saving an image with imwrite()\n",
        "\n",
        "For a more comprehensive study of OpenCV, we encourage you to check the official [OpenCV documentation](https://docs.opencv.org/master/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB-PYVl-SOkm",
        "outputId": "385b06d4-6b89-49ec-c996-da830862215a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwKOtbVsc8C8",
        "outputId": "679a1bbe-ec93-4833-b584-a1a1c22f16f5"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device  :',device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device  : cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7ZvSiY3qW_U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN5WJ_XG-Sso",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "e2caff02-6bec-482a-9540-8af8e8bdef6d"
      },
      "source": [
        "\n",
        "import os\n",
        "import cv2 #import OpenCVfrom matplotlib import pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def show_image_mask(img, mask, cmap='gray'): # visualisation\n",
        "    fig = plt.figure(figsize=(5,5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img, cmap=cmap)\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(mask, cmap=cmap)\n",
        "    plt.axis('off')\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/data/train/'\n",
        "image = cv2.imread(os.path.join(data_dir,'image','cmr1.png'), cv2.IMREAD_UNCHANGED)\n",
        "mask = cv2.imread(os.path.join(data_dir,'mask','cmr1_mask.png'), cv2.IMREAD_UNCHANGED)\n",
        "show_image_mask(image, mask, cmap='gray')\n",
        "plt.pause(1)\n",
        "cv2.imwrite(os.path.join('./','cmr1.png'), mask*85)\n",
        "print(image.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UDvsGZnYfHS"
      },
      "source": [
        "Note: You will no doubt notice that the mask images appear to be completely black with no sign of any segmentations. This is because the max intensity of pixels in an 8-bit png image is 255 and your image viewer software only sees 255 as white. For those values close to zero, you will only see dark values. This is the case for our masks as the background, the right ventricle, the myocardium, and the left ventricle in each image are 0, 1, 2, and 3, respectively. All of which are close to zero. If we multiply the original mask by 85 and save the result to the directory where this code is, we can see the heart indeed shows up. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hULAX3WH-Sss"
      },
      "source": [
        "## 2 Define a segmentation model with Pytorch\n",
        "\n",
        "In this section, we expect you to learn how to:\n",
        "* Define a Segmentation Model\n",
        "* Define a DataLoader that inputs images to the Model\n",
        "* Define training parameters and train the model\n",
        "* Test the trained model with a new input image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrKFgoZvUbeg"
      },
      "source": [
        "### 2.1 Define a DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC9s43MqqW_U"
      },
      "source": [
        "Below we provide you with a dataloader to use in your assigment. You will only need to focus on the development of your model and loss function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYrD95T8qz8T"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import cv2\n",
        "import os\n",
        "from glob import glob\n",
        "path = '/content/drive/MyDrive/data/train'\n",
        "\n",
        "\n",
        "class TrainDataset(data.Dataset):\n",
        "    def __init__(self, root=path):\n",
        "        super(TrainDataset, self).__init__()\n",
        "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
        "        #print(self.img_files)\n",
        "        self.mask_files = []\n",
        "        #print(self.img_files)\n",
        "        for img_path in self.img_files:\n",
        "            basename = os.path.basename(img_path)\n",
        "            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))\n",
        "            \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "            img_path = self.img_files[index]\n",
        "            mask_path = self.mask_files[index]\n",
        "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
        "            return torch.from_numpy(data).float(), torch.from_numpy(label).float()    #tensor vlaues for data( image) and label( mask) is returned.\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "class TestDataset(data.Dataset):\n",
        "    def __init__(self, root=''):\n",
        "        super(TestDataset, self).__init__()\n",
        "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "            img_path = self.img_files[index]\n",
        "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "            return torch.from_numpy(data).float(),img_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82UAfnwSUgc_"
      },
      "source": [
        "### 2.2 Define a Segmenatation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEIkCqdfYnIn"
      },
      "source": [
        "You will need to define your CNN model for segmentation below\n",
        "\n",
        "The model used is UNEt with modifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W6532hFXa_g"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def double_conv(in_channels, out_channels, mid_channels = None):\n",
        "    if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "    return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "    )   \n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, n_class=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dconv_pre = double_conv(1,3)       \n",
        "        self.dconv_down1 = double_conv(3, 64)\n",
        "        self.dconv_down2 = double_conv(64, 128)\n",
        "        self.dconv_down3 = double_conv(128, 256)\n",
        "        self.dconv_down4 = double_conv(256, 512) \n",
        "       #self.dconv_down5 = double_conv(512, 1024)       \n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
        "        \n",
        "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
        "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
        "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
        "        \n",
        "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        convpre = self.dconv_pre(x) #not maxpooling here\n",
        "        conv1 = self.dconv_down1(convpre)\n",
        "        x = self.maxpool(conv1)\n",
        "\n",
        "        conv2 = self.dconv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "        \n",
        "        conv3 = self.dconv_down3(x)\n",
        "        x = self.maxpool(conv3)   \n",
        "        \n",
        "        x = self.dconv_down4(x)\n",
        "        \n",
        "        x = self.upsample(x)        \n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "        \n",
        "        x = self.dconv_up3(x)\n",
        "        x = self.upsample(x)        \n",
        "        x = torch.cat([x, conv2], dim=1)       \n",
        "\n",
        "        x = self.dconv_up2(x)\n",
        "        x = self.upsample(x)        \n",
        "        x = torch.cat([x, conv1], dim=1)   \n",
        "        \n",
        "        x = self.dconv_up1(x)\n",
        "        \n",
        "        out = self.conv_last(x)\n",
        "        \n",
        "        return out\n",
        "\"\"\"\n",
        "class CNNSEG(nn.Module):\n",
        "\n",
        "    def __init__(self, n_class=4):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.dconv_down1 = double_conv(1, 3)\n",
        "        self.dconv_down2 = double_conv(3, 64)\n",
        "        self.dconv_down3 = double_conv(64, 128)\n",
        "        self.dconv_down4 = double_conv(128, 256)\n",
        "        self.dconv_down5 = double_conv(256, 512)          \n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2) #kernel size two..stride = none\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
        "        \n",
        "        \n",
        "       # self.dconv_up3 = double_conv(256 + 512, 256)\n",
        "        #self.dconv_up2 = double_conv(128 + 256, 128)\n",
        "        #self.dconv_up1 = double_conv(128 + 64, 64)\n",
        "        self.dconv_up4 = double_conv(512+512, 512)\n",
        "        self.dconv_up3 = double_conv()\n",
        "        self.dconv_up2 = double_conv(512+512, 512)\n",
        "        self.dconv_up1 = double_conv(512+512, 512)\n",
        "        \n",
        "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        conv1 = self.dconv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "\n",
        "        conv2 = self.dconv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "        \n",
        "        conv3 = self.dconv_down3(x)\n",
        "        x = self.maxpool(conv3)   \n",
        "        \n",
        "        conv4 = self.dconv_down4(x)\n",
        "        x = self.maxpool(conv4) \n",
        "\n",
        "        conv5 = self.dconv_down5(x)\n",
        "\n",
        "        x = self.upsample(conv5)        #left with 512 channels and ..becomes 12 * 12\n",
        "        x = torch.cat([x, conv4], dim=1)\n",
        "        \n",
        "        x = self.dconv_up3(x)\n",
        "        x = self.upsample(x)        \n",
        "        x = torch.cat([x, conv3], dim=1)       \n",
        "\n",
        "        x = self.dconv_up2(x)\n",
        "        x = self.upsample(x)        \n",
        "        x = torch.cat([x, conv2], dim=1)   \n",
        "        \n",
        "        x = self.dconv_up1(x)\n",
        "        \n",
        "        out = self.conv_last(x)\n",
        "        \n",
        "        return out\n",
        "\"\"\"\n",
        "\n",
        "#https://github.com/usuyama/pytorch-unet/blob/master/pytorch_unet.py\n",
        "\n",
        "model = UNet() # We can now create a model using your defined segmentation model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUryRrRpTJpq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def double_conv(in_channels, out_channels, mid_channels = None):\n",
        "    if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "    return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "    )   \n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, n_class=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dconv_pre = nn.Sequential(nn.Conv2d(1, 3, kernel_size=3, padding=1),\n",
        "                                       nn.BatchNorm2d(3),\n",
        "                                       nn.ReLU(inplace=True))      \n",
        "        self.dconv_down1 = double_conv(3, 64)\n",
        "        self.dconv_down2 = double_conv(64, 128)\n",
        "        self.dconv_down3 = double_conv(128, 256)\n",
        "        self.dconv_down4 = double_conv(256, 512) \n",
        "       #self.dconv_down5 = double_conv(512, 1024)       \n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
        "        \n",
        "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
        "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
        "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
        "        \n",
        "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        convpre = self.dconv_pre(x) #not maxpooling here\n",
        "        conv1 = self.dconv_down1(convpre)\n",
        "        x = self.maxpool(conv1)\n",
        "\n",
        "        conv2 = self.dconv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "        \n",
        "        conv3 = self.dconv_down3(x)\n",
        "        x = self.maxpool(conv3)   \n",
        "        \n",
        "        x = self.dconv_down4(x)\n",
        "        \n",
        "        x = self.upsample(x)        \n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "        \n",
        "        x = self.dconv_up3(x)\n",
        "        x = self.upsample(x)        \n",
        "        x = torch.cat([x, conv2], dim=1)       \n",
        "\n",
        "        x = self.dconv_up2(x)\n",
        "        x = self.upsample(x)        \n",
        "        x = torch.cat([x, conv1], dim=1)   \n",
        "        \n",
        "        x = self.dconv_up1(x)\n",
        "        \n",
        "        out = self.conv_last(x)\n",
        "        \n",
        "        return out\n",
        "\n",
        "model = UNet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8JgA8I1VxIJ",
        "outputId": "b97cc343-0194-4a26-bd9d-777cb35ae254"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (dconv_pre): Sequential(\n",
              "    (0): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (dconv_down1): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "  )\n",
              "  (dconv_down2): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "  )\n",
              "  (dconv_down3): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "  )\n",
              "  (dconv_down4): Sequential(\n",
              "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "  )\n",
              "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
              "  (dconv_up3): Sequential(\n",
              "    (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "  )\n",
              "  (dconv_up2): Sequential(\n",
              "    (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "  )\n",
              "  (dconv_up1): Sequential(\n",
              "    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv_last): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXfeO8wDWV6S",
        "outputId": "0916fcfe-d644-4063-c6db-975b8c853f2b"
      },
      "source": [
        "from torchsummary import  summary\n",
        "#help to view the visualisation of the model\n",
        "\n",
        "summary(model,input_size=(1,96,96))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 3, 96, 96]              30\n",
            "       BatchNorm2d-2            [-1, 3, 96, 96]               6\n",
            "              ReLU-3            [-1, 3, 96, 96]               0\n",
            "            Conv2d-4           [-1, 64, 96, 96]           1,792\n",
            "       BatchNorm2d-5           [-1, 64, 96, 96]             128\n",
            "              ReLU-6           [-1, 64, 96, 96]               0\n",
            "            Conv2d-7           [-1, 64, 96, 96]          36,928\n",
            "       BatchNorm2d-8           [-1, 64, 96, 96]             128\n",
            "              ReLU-9           [-1, 64, 96, 96]               0\n",
            "        MaxPool2d-10           [-1, 64, 48, 48]               0\n",
            "           Conv2d-11          [-1, 128, 48, 48]          73,856\n",
            "      BatchNorm2d-12          [-1, 128, 48, 48]             256\n",
            "             ReLU-13          [-1, 128, 48, 48]               0\n",
            "           Conv2d-14          [-1, 128, 48, 48]         147,584\n",
            "      BatchNorm2d-15          [-1, 128, 48, 48]             256\n",
            "             ReLU-16          [-1, 128, 48, 48]               0\n",
            "        MaxPool2d-17          [-1, 128, 24, 24]               0\n",
            "           Conv2d-18          [-1, 256, 24, 24]         295,168\n",
            "      BatchNorm2d-19          [-1, 256, 24, 24]             512\n",
            "             ReLU-20          [-1, 256, 24, 24]               0\n",
            "           Conv2d-21          [-1, 256, 24, 24]         590,080\n",
            "      BatchNorm2d-22          [-1, 256, 24, 24]             512\n",
            "             ReLU-23          [-1, 256, 24, 24]               0\n",
            "        MaxPool2d-24          [-1, 256, 12, 12]               0\n",
            "           Conv2d-25          [-1, 512, 12, 12]       1,180,160\n",
            "      BatchNorm2d-26          [-1, 512, 12, 12]           1,024\n",
            "             ReLU-27          [-1, 512, 12, 12]               0\n",
            "           Conv2d-28          [-1, 512, 12, 12]       2,359,808\n",
            "      BatchNorm2d-29          [-1, 512, 12, 12]           1,024\n",
            "             ReLU-30          [-1, 512, 12, 12]               0\n",
            "         Upsample-31          [-1, 512, 24, 24]               0\n",
            "           Conv2d-32          [-1, 256, 24, 24]       1,769,728\n",
            "      BatchNorm2d-33          [-1, 256, 24, 24]             512\n",
            "             ReLU-34          [-1, 256, 24, 24]               0\n",
            "           Conv2d-35          [-1, 256, 24, 24]         590,080\n",
            "      BatchNorm2d-36          [-1, 256, 24, 24]             512\n",
            "             ReLU-37          [-1, 256, 24, 24]               0\n",
            "         Upsample-38          [-1, 256, 48, 48]               0\n",
            "           Conv2d-39          [-1, 128, 48, 48]         442,496\n",
            "      BatchNorm2d-40          [-1, 128, 48, 48]             256\n",
            "             ReLU-41          [-1, 128, 48, 48]               0\n",
            "           Conv2d-42          [-1, 128, 48, 48]         147,584\n",
            "      BatchNorm2d-43          [-1, 128, 48, 48]             256\n",
            "             ReLU-44          [-1, 128, 48, 48]               0\n",
            "         Upsample-45          [-1, 128, 96, 96]               0\n",
            "           Conv2d-46           [-1, 64, 96, 96]         110,656\n",
            "      BatchNorm2d-47           [-1, 64, 96, 96]             128\n",
            "             ReLU-48           [-1, 64, 96, 96]               0\n",
            "           Conv2d-49           [-1, 64, 96, 96]          36,928\n",
            "      BatchNorm2d-50           [-1, 64, 96, 96]             128\n",
            "             ReLU-51           [-1, 64, 96, 96]               0\n",
            "           Conv2d-52            [-1, 4, 96, 96]             260\n",
            "================================================================\n",
            "Total params: 7,788,776\n",
            "Trainable params: 7,788,776\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.04\n",
            "Forward/backward pass size (MB): 116.51\n",
            "Params size (MB): 29.71\n",
            "Estimated Total Size (MB): 146.25\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRdPFTa9a34J"
      },
      "source": [
        "### 2.3 Define a Loss function and optimizer\n",
        "\n",
        "You will need to define a loss function and an optimizer. torch.nn has a variety of readymade loss functions, although you may wish to create your own instead. torch.optim has a variety of optimizers, it is advised that you use one of these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRjOZGXRbUFT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ee8af236-d89b-4cdb-9e28-f9f29523263c"
      },
      "source": [
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "\"\"\"\n",
        "def dice_loss(pred,target, smooth = 1.):\n",
        "\n",
        "    pred = pred.contiguous()\n",
        "    target = target.contiguous()    \n",
        "    \n",
        "    #When you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\n",
        "\n",
        "    #Normally you don't need to worry about this. You're generally safe to assume everything will work, and wait until you get a RuntimeError: input is not contiguous where PyTorch expects a contiguous tensor to add a call to contiguous().\n",
        "\n",
        "    \n",
        "\n",
        "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
        "    \n",
        "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
        "    \n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# so we have two types of loss the dice loss and cross entropy loss\n",
        "\n",
        "def optimizer():\n",
        "  return optim.Adam(filter( lambda p:p.requires_grad,model.parameters()),lr=1e-4)\n",
        "\n",
        "optimizer_ft = optimizer()\n",
        "def exp_lr_scheduler():\n",
        "  return lr_scheduler.StepLR(optimizer_ft,step_size=30,gamma=0.1)\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef dice_loss(pred,target, smooth = 1.):\\n\\n    pred = pred.contiguous()\\n    target = target.contiguous()    \\n    \\n    #When you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\\n\\n    #Normally you don't need to worry about this. You're generally safe to assume everything will work, and wait until you get a RuntimeError: input is not contiguous where PyTorch expects a contiguous tensor to add a call to contiguous().\\n\\n    \\n\\n    intersection = (pred * target).sum(dim=2).sum(dim=2)\\n    \\n    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\\n    \\n    return loss.mean()\\n\\n\\ncriterion = nn.CrossEntropyLoss()\\n\\n# so we have two types of loss the dice loss and cross entropy loss\\n\\ndef optimizer():\\n  return optim.Adam(filter( lambda p:p.requires_grad,model.parameters()),lr=1e-4)\\n\\noptimizer_ft = optimizer()\\ndef exp_lr_scheduler():\\n  return lr_scheduler.StepLR(optimizer_ft,step_size=30,gamma=0.1)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grDz3fR1qW_V"
      },
      "source": [
        "### 2.4 Training\n",
        "\n",
        "As most of you will use CPUs to train the model, expect your models to take **30 minutes to train if not longer depending on network architecture**. To save time, you should not be using all training data until your model is well developed. If you are running your model on a GPU training should be significantly faster. During the training process, you may want to save the checkpoints as follows:\n",
        "\n",
        "```\n",
        "# Saving checkpoints for validation/testing\n",
        "torch.save(model.state_dict(), path)\n",
        "```\n",
        "The saved checkpoints can be used to load at a later date for validation and testing. Here we give some example code for training a model. Note that you need to specify the max iterations you want to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrFNs1t6PjNR"
      },
      "source": [
        "# https://github.com/milesial/Pytorch-UNet/blob/master/utils/dice_score.py\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
        "    # Average of Dice coefficient for all batches, or for a single mask\n",
        "    assert input.size() == target.size()\n",
        "    if input.dim() == 2 and reduce_batch_first:\n",
        "        raise ValueError(f'Dice: asked to reduce batch but got tensor without batch dimension (shape {input.shape})')\n",
        "\n",
        "    if input.dim() == 2 or reduce_batch_first:\n",
        "        inter = torch.dot(input.reshape(-1), target.reshape(-1))\n",
        "        sets_sum = torch.sum(input) + torch.sum(target)\n",
        "        if sets_sum.item() == 0:\n",
        "            sets_sum = 2 * inter\n",
        "\n",
        "        return (2 * inter + epsilon) / (sets_sum + epsilon)\n",
        "    else:\n",
        "        # compute and average metric for each batch element\n",
        "        dice = 0\n",
        "        for i in range(input.shape[0]):\n",
        "            dice += dice_coeff(input[i, ...], target[i, ...])\n",
        "        return dice / input.shape[0]\n",
        "\n",
        "\n",
        "def multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
        "    # Average of Dice coefficient for all classes\n",
        "    assert input.size() == target.size()\n",
        "    dice = 0\n",
        "    for channel in range(input.shape[1]):\n",
        "        dice += dice_coeff(input[:, channel, ...], target[:, channel, ...], reduce_batch_first, epsilon)\n",
        "\n",
        "    return dice / input.shape[1]\n",
        "\n",
        "\n",
        "def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n",
        "    # Dice loss (objective to minimize) between 0 and 1\n",
        "    assert input.size() == target.size()\n",
        "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
        "    return 1 - fn(input, target, reduce_batch_first=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CalId4BChXvD"
      },
      "source": [
        "model = UNet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCb4bxVVchxf"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\"\"\"\n",
        "def print_metrics(metrics, epoch_samples, phase='train'):\n",
        "    outputs = []\n",
        "    for k in metrics.keys():\n",
        "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
        "\n",
        "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
        "\n",
        "def calc_loss(pred,target,metrics,bce_weight=0.5) :\n",
        "    #bce = F.binary_cross_entropy_with_logits(pred,target)\n",
        "\n",
        "    pred = torch.sigmoid(pred)\n",
        "    dice = dice_loss(pred,target) \n",
        "\n",
        "    #loss = bce * bce_weight + dice * (1 - bce_weight)\n",
        "    \n",
        "\n",
        "    metrics['bce'] += 1#bce.data.cpu().numpy() * target.size(0)\n",
        "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
        "    metrics['loss'] += 1#loss.data.cpu().numpy() * target.size(0)\n",
        "\n",
        "    return metrics['loss']\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "def dice_coef_binary_4cat(y_true, y_pred, smooth=1e-7):\n",
        "    '''\n",
        "    Dice coefficient for 4 categories. Ignores background pixel label 0\n",
        "    Pass to model as metric during compile statement\n",
        "    '''\n",
        "    y_true_f = torch.flatten(F.one_hot(torch.cast(y_true, 'int32'), num_classes=4)[...,1:])\n",
        "    y_pred_f = torch.flatten(y_pred[...,1:])\n",
        "    intersect = F.sum(y_true_f * y_pred_f, axis=-1)\n",
        "    denom = F.sum(y_true_f + y_pred_f, axis=-1)\n",
        "    return F.mean((2. * intersect / (denom + smooth)))\n",
        "\n",
        "\n",
        "def dice_coef_binary_loss(y_true, y_pred):\n",
        "    '''\n",
        "    Dice loss to minimize. Pass to model as loss during compile statement\n",
        "    '''\n",
        "    return 1 - dice_coef_binary_4cat(y_true, y_pred)\n",
        "\n",
        "\"\"\"\n",
        "checkpoint_path = \"checkpoint.pth\"\n",
        "data_path = '/content/drive/MyDrive/data/train/'\n",
        "num_workers = 4\n",
        "batch_size = 4\n",
        "\"\"\"\n",
        "#what does data loader do?\n",
        "Dataset stores the samples and their corresponding labels, and \n",
        "DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
        "\n",
        "\"\"\"\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=1e-8, momentum=0.9)\n",
        "optimizer_fn = optim.Adam(model.parameters(),lr = 3e-4)\n",
        "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)  # goal: maximize Dice score\n",
        "scheduler_fn = lr_scheduler.StepLR(optimizer_fn,step_size=30,gamma=0.1)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "if isinstance(loss_fn,torch.nn.Module): ##just to be sure that criterion is on the correct device.\n",
        "  loss_fn.to(device)\n",
        "\n",
        "def train_net(model,\n",
        "              device,\n",
        "              epochs: int =10,\n",
        "              batch_size: int = 1,\n",
        "              learning_rate: float = 3e-4,\n",
        "              optimizer = optimizer_fn,\n",
        "              scheduler = scheduler_fn,\n",
        "              criterion = loss_fn,\n",
        "              #val_percent: float = 0.1,\n",
        "              #save_checkpoint: bool = True,\n",
        "             # img_scale: float = 0.5,\n",
        "              amp: bool = False\n",
        "):\n",
        "#print(\"training the network...\")\n",
        "# Fetch images and labels.\n",
        "  model.train()  #funciton of the module superclass ..sets the model to train.\n",
        "  train_set = TrainDataset(data_path)\n",
        "  training_data_loader = DataLoader(dataset=train_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
        " # https://github.com/milesial/Pytorch-UNet/blob/master/train.py \n",
        "\n",
        "\n",
        "  global_step = 0\n",
        "  steps =[]\n",
        "  losses=[]\n",
        "  for epoch in range(epochs) :\n",
        "    #torch.set_grad_enabled(True)\n",
        "    epoch_samples = 0\n",
        "    epoch_loss=0\n",
        "    metrics = defaultdict(float)\n",
        "    since = time.time()\n",
        "  \n",
        "    with tqdm(len(train_set), desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:     \n",
        "      for iteration, sample in enumerate(training_data_loader):\n",
        "      #for batch in training_data_loader:\n",
        "          img, mask = sample\n",
        "\n",
        "          #print('\\nimage size :', img.shape)\n",
        "          print('mask size :',mask.shape)\n",
        "          img=img.unsqueeze(1)\n",
        "          print('\\nimage size :', img.shape)\n",
        "          img  = img.to(device = device)\n",
        "          #https://discuss.pytorch.org/t/only-batches-of-spatial-targets-supported-non-empty-3d-tensors-but-got-targets-of-size-1-1-256-256/49134/18 \n",
        "          mask = mask.squeeze(1)\n",
        "          mask = mask.to(device,dtype = torch.long)\n",
        "\n",
        "      \n",
        "          #mask = F.one_hot(mask,4).permute(0,3,1,2).float()\n",
        "          #print(mask.shape)\n",
        "          \"\"\"\n",
        "          torch.cuda.amp and torch provide convenience methods for mixed precision,\n",
        "          where some operations use the torch.float32 (float) datatype and other operations use torch.float16 (half).\n",
        "          Some ops, like linear layers and convolutions, are much faster in float16. \n",
        "          Other ops, like reductions, often require the dynamic range of float32. \n",
        "          Mixed precision tries to match each op to its appropriate datatype.\n",
        "          \"\"\"\n",
        "          #with torch.cuda.amp.autocast(enabled = amp)\n",
        "          out  = model(img)\n",
        "          #print(out.shape)\n",
        "          optimizer.zero_grad()\n",
        "          cross_entropy_loss = criterion (out,mask) \n",
        "          #dice_loss = dice_coef_binary_loss(mask,out)\n",
        "          dice= dice_loss(F.softmax(out, dim=1).float(), F.one_hot(mask, 4).permute(0, 3, 1, 2).float(),multiclass=True)\n",
        "          loss = dice\n",
        "          #loss = cross_entropy_loss\n",
        "    \n",
        "    # Then write your BACKWARD & OPTIMIZE below\n",
        "    # Note: Compute Loss and Optimize\n",
        "    #loss = criterion(out,mask)\n",
        "    #loss_value = loss.item()\n",
        "    #train_losses.append(loss_value)\n",
        "    #loss.backward()\n",
        "          \n",
        "\n",
        "         # optimizer.zero_grad(set_to_none=True)\n",
        "         # grad_scaler.scale(loss).backward()\n",
        "         # grad_scaler.step(optimizer)\n",
        "         # grad_scaler.update()\n",
        "\n",
        "          pbar.update(img.shape[0])\n",
        "          global_step += 1\n",
        "          epoch_loss += loss.item()\n",
        "          losses.append(loss.item())\n",
        "          steps.append(global_step)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          #experiment.log({\n",
        "           #         'train loss': loss.item(),\n",
        "            #        'step': global_step,\n",
        "             #       'epoch': epoch\n",
        "               # })\n",
        "          print('train loss',loss.item(),' step :',global_step,' epoch :',epoch)\n",
        "          pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
        "  plt.plot(steps[1::2],losses[1::2],'.r-')\n",
        "  plt.xlabel(\"step\")\n",
        "  plt.ylabel(\"loss\")\n",
        "  x1,x2,y1,y2 = plt.axis()  \n",
        "  plt.axis((x1,x2,0,1))\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(18.5, 10.5, forward=True)\n",
        "  fig.set_dpi(100)\n",
        "  plt.show()\n",
        "  return model, losses\n",
        "model, losses = train_net(model,device)\n",
        "torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghl4sMHbALUP"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li4hMIa7AQnR",
        "outputId": "87bdaea1-6dda-410c-e8c6-ba0363774277"
      },
      "source": [
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lykP0dIoATA3"
      },
      "source": [
        "# In this block you are expected to write code to load saved model and deploy it to all data in test set to \n",
        "# produce segmentation masks in png images valued 0,1,2,3, which will be used for the submission to Kaggle.\n",
        "Learned_model = model\n",
        "\n",
        "num_workers = 4\n",
        "batch_size = 2\n",
        "loss_list = []\n",
        "Learned_model.to(device)  #Change\n",
        "val_set = TrainDataset(data_path)  #Change\n",
        "data_path = '/content/drive/MyDrive/data/train/'\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "Learned_model.eval()  #Change\n",
        "\n",
        "for image,mask in test_data_loader:\n",
        "        #print(image.shape,mask.shape)\n",
        "        image = image.to(device)\n",
        "        image = image.unsqueeze(1)\n",
        "        \n",
        "        mask = mask.to(device,dtype = torch.long)\n",
        "        #####PLEASE COMPLETE THE FOLLOWING CODE#####\n",
        "        #insert the neccessary lines of code to complete the training loop here \n",
        "        #(approx 5 lines of code)\n",
        "        ############################################\n",
        "        out = Learned_model(image)\n",
        "       # loss = criterion(out,mask.long())\n",
        "        loss = dice_loss(F.softmax(out, dim=1).float(), F.one_hot(mask, 4).permute(0, 3, 1, 2).float(),multiclass=True)\n",
        "        print('loss score for image : ',loss.item())\n",
        "        loss_list.append(loss.item())\n",
        "        out_np = torch.max(out,1).indices.cpu().detach().numpy()\n",
        "        mask_np = mask.cpu().detach().numpy()\n",
        "        image_np = image.cpu().detach().numpy()\n",
        "        print(out_np[0].shape,mask_np.shape)\n",
        "        print(image_np[0].shape,mask_np.shape)\n",
        "        for i in range(1):\n",
        "            show_image_mask(image_np[i,0], out_np[i], cmap='gray')\n",
        "            plt.pause(1)\n",
        "print(loss_list)\n",
        "#for iteration, sample in enumerate(test_data_loader):\n",
        "#   img = sample\n",
        "#   print(img.shape)\n",
        "    \n",
        "#    plt.imshow(img[0,...].squeeze(), cmap='gray') #visualise all images in test set\n",
        "#    plt.pause(1)\n",
        "#model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8ZH_D_oCGxT"
      },
      "source": [
        "# Main code Test + Val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpvb2a1CkA2R",
        "outputId": "af7098a9-fccf-4007-d76d-2885b5d08a0f"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import argparse\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "val_data_path = '/content/drive/MyDrive/data/val'\n",
        "data_path = '/content/drive/MyDrive/data/val'\n",
        "\n",
        "num_workers = 4\n",
        "batch_size = 2\n",
        "epoch_train_losses=[]\n",
        "epoch_val_losses = []\n",
        "\n",
        "val_set = TrainDataset(val_data_path)  #Change\n",
        "val_data_loader = DataLoader(dataset=val_set, num_workers=num_workers,batch_size=batch_size, shuffle=False)\n",
        "train_set = TrainDataset(data_path)\n",
        "training_data_loader = DataLoader(dataset=train_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=1e-8, momentum=0.9)\n",
        "optimizer_fn = optim.Adam(model.parameters(),lr = 3e-4)\n",
        "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)  # goal: maximize Dice score\n",
        "scheduler_fn = lr_scheduler.StepLR(optimizer_fn,step_size=30,gamma=0.1)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "if isinstance(loss_fn,torch.nn.Module): ##just to be sure that criterion is on the correct device.\n",
        "  loss_fn.to(device)\n",
        "\n",
        "def train_and_validate(model,\n",
        "              device,\n",
        "              epochs: int =10,\n",
        "              batch_size: int = 1,\n",
        "              learning_rate: float = 3e-4,\n",
        "              optimizer = optimizer_fn,\n",
        "              scheduler = scheduler_fn,\n",
        "              criterion = loss_fn,\n",
        "              #val_percent: float = 0.1,\n",
        "              #save_checkpoint: bool = True,\n",
        "             # img_scale: float = 0.5,\n",
        "              amp: bool = False\n",
        "):\n",
        "  \n",
        "  for epoch in range(epochs) :\n",
        "    #torch.set_grad_enabled(True)\n",
        "\n",
        "    since = time.time()\n",
        "     #with tqdm(len(train_set), desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:     \n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    for iteration, sample in enumerate(training_data_loader):\n",
        "    #for batch in training_data_loader:\n",
        "        img, mask = sample\n",
        "\n",
        "  \n",
        "        \n",
        "        img=img.unsqueeze(1)\n",
        "        img  = img.to(device = device)\n",
        "        #https://discuss.pytorch.org/t/only-batches-of-spatial-targets-supported-non-empty-3d-tensors-but-got-targets-of-size-1-1-256-256/49134/18 \n",
        "        mask = mask.squeeze(1)\n",
        "        mask = mask.to(device,dtype = torch.long)\n",
        "\n",
        "      \n",
        "         \n",
        "        out  = model(img)\n",
        "          #print(out.shape)\n",
        "        optimizer.zero_grad()\n",
        "        cross_entropy_loss = criterion (out,mask) \n",
        "          #dice_loss = dice_coef_binary_loss(mask,out)\n",
        "        dice= dice_loss(F.softmax(out, dim=1).float(), F.one_hot(mask, 4).permute(0, 3, 1, 2).float(),multiclass=True)\n",
        "        #loss = dice\n",
        "        loss = cross_entropy_loss\n",
        "    \n",
        "    # Then write your BACKWARD & OPTIMIZE below\n",
        "    # Note: Compute Loss and Optimize\n",
        "    #loss = criterion(out,mask)\n",
        "    #loss_value = loss.item()\n",
        "    #train_losses.append(loss_value)\n",
        "    #loss.backward()\n",
        "          \n",
        "\n",
        "         # optimizer.zero_grad(set_to_none=True)\n",
        "         # grad_scaler.scale(loss).backward()\n",
        "         # grad_scaler.step(optimizer)\n",
        "         # grad_scaler.update()\n",
        "\n",
        "          #pbar.update(img.shape[0])\n",
        "       \n",
        "        \n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "     #   print('train loss',loss.item(),' step :',global_step,' epoch :',epoch)\n",
        "    \n",
        "    val_loss =0.0\n",
        "    model.eval()\n",
        "    for image,mask in val_data_loader:\n",
        "      #print(image.shape,mask.shape)\n",
        "      image = image.to(device)\n",
        "      image = image.unsqueeze(1)\n",
        "        \n",
        "      mask = mask.to(device,dtype = torch.long)\n",
        "        #####PLEASE COMPLETE THE FOLLOWING CODE#####\n",
        "        #insert the neccessary lines of code to complete the training loop here \n",
        "        #(approx 5 lines of code)\n",
        "        ############################################\n",
        "      out = model(image)\n",
        "      loss = criterion(out,mask.long())\n",
        "      #loss = dice_loss(F.softmax(out, dim=1).float(), F.one_hot(mask, 4).permute(0, 3, 1, 2).float(),multiclass=True)\n",
        "      val_loss += loss.item() * image.size(0)\n",
        "      out_np = torch.max(out,1).indices.cpu().detach().numpy()\n",
        "      #mask_np = mask.cpu().detach().numpy()\n",
        "      #image_np = image.cpu().detach().numpy()\n",
        "      #print(out_np[0].shape,mask_np.shape)\n",
        "      #print(image_np[0].shape,mask_np.shape)\n",
        "      #print('Val loss',loss_list.item(),' step :',global_step,' epoch :',epoch)\n",
        "      #for i in range(1):\n",
        "          #show_image_mask(image_np[i,0], out_np[i], cmap='gray')\n",
        "          #plt.pause(1)\n",
        "    epoch_train_losses.append(train_loss / len(train_set))\n",
        "    epoch_val_losses.append(val_loss/len(val_set))\n",
        "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(train_set)} \\t\\t Validation Loss: {val_loss / len(val_set)}')\n",
        "    \n",
        "  return model\n",
        "\n",
        "model =  train_and_validate(model,device,30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \t\t Training Loss: 0.5308473914861679 \t\t Validation Loss: 1.2195829033851624\n",
            "Epoch 2 \t\t Training Loss: 0.3484025627374649 \t\t Validation Loss: 1.0112667798995971\n",
            "Epoch 3 \t\t Training Loss: 0.2926463097333908 \t\t Validation Loss: 0.7242559671401978\n",
            "Epoch 4 \t\t Training Loss: 0.25780790746212007 \t\t Validation Loss: 0.6995344400405884\n",
            "Epoch 5 \t\t Training Loss: 0.22858039885759354 \t\t Validation Loss: 0.49097307324409484\n",
            "Epoch 6 \t\t Training Loss: 0.19432607889175416 \t\t Validation Loss: 0.39546737968921664\n",
            "Epoch 7 \t\t Training Loss: 0.1725316122174263 \t\t Validation Loss: 0.3514880478382111\n",
            "Epoch 8 \t\t Training Loss: 0.15596081763505937 \t\t Validation Loss: 0.29465358555316923\n",
            "Epoch 9 \t\t Training Loss: 0.14165190756320953 \t\t Validation Loss: 0.3004500806331635\n",
            "Epoch 10 \t\t Training Loss: 0.13103242218494415 \t\t Validation Loss: 0.25059657990932466\n",
            "Epoch 11 \t\t Training Loss: 0.11879841163754463 \t\t Validation Loss: 0.21621857285499574\n",
            "Epoch 12 \t\t Training Loss: 0.10698643326759338 \t\t Validation Loss: 0.19643683433532716\n",
            "Epoch 13 \t\t Training Loss: 0.0971677415072918 \t\t Validation Loss: 0.1807894304394722\n",
            "Epoch 14 \t\t Training Loss: 0.0891661748290062 \t\t Validation Loss: 0.1643799662590027\n",
            "Epoch 15 \t\t Training Loss: 0.08434978350996972 \t\t Validation Loss: 0.15708985626697541\n",
            "Epoch 16 \t\t Training Loss: 0.0776341587305069 \t\t Validation Loss: 0.14300709068775178\n",
            "Epoch 17 \t\t Training Loss: 0.07461138889193535 \t\t Validation Loss: 0.1353789821267128\n",
            "Epoch 18 \t\t Training Loss: 0.07046831473708152 \t\t Validation Loss: 0.12437106817960739\n",
            "Epoch 19 \t\t Training Loss: 0.06337199881672859 \t\t Validation Loss: 0.1258505716919899\n",
            "Epoch 20 \t\t Training Loss: 0.061272118240594864 \t\t Validation Loss: 0.1151270791888237\n",
            "Epoch 21 \t\t Training Loss: 0.056499289348721504 \t\t Validation Loss: 0.11072256267070771\n",
            "Epoch 22 \t\t Training Loss: 0.05388153046369552 \t\t Validation Loss: 0.0987833134829998\n",
            "Epoch 23 \t\t Training Loss: 0.05074327141046524 \t\t Validation Loss: 0.09164592698216438\n",
            "Epoch 24 \t\t Training Loss: 0.047250131890177724 \t\t Validation Loss: 0.08978659361600876\n",
            "Epoch 25 \t\t Training Loss: 0.04582459405064583 \t\t Validation Loss: 0.08726694732904434\n",
            "Epoch 26 \t\t Training Loss: 0.045248443633317946 \t\t Validation Loss: 0.08314919620752334\n",
            "Epoch 27 \t\t Training Loss: 0.04259488396346569 \t\t Validation Loss: 0.07895732671022415\n",
            "Epoch 28 \t\t Training Loss: 0.04044999852776528 \t\t Validation Loss: 0.08020432963967324\n",
            "Epoch 29 \t\t Training Loss: 0.04013413302600384 \t\t Validation Loss: 0.07396891191601754\n",
            "Epoch 30 \t\t Training Loss: 0.038492539152503016 \t\t Validation Loss: 0.0779114693403244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "1xx7yXwchdMP",
        "outputId": "4285481b-8d22-4066-b36d-f9c5def99056"
      },
      "source": [
        "\n",
        "plt.plot(epoch_train_losses)\n",
        "plt.plot(epoch_val_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa631538850>]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCZP-xof-Sst"
      },
      "source": [
        "### 2.5 Testing\n",
        "\n",
        "When validating the trained checkpoints (models), remember to change the model status as **Evaluation Mode**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGmhTdkciDt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784aae1f-741a-420a-83fb-8df767d8f370"
      },
      "source": [
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVS22lrjqW_V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "811ae80e-b37d-488b-e59e-7062a018cdc4"
      },
      "source": [
        "# In this block you are expected to write code to load saved model and deploy it to all data in test set to \n",
        "# produce segmentation masks in png images valued 0,1,2,3, which will be used for the submission to Kaggle.\n",
        "import re\n",
        "data_path = '/content/drive/MyDrive/data/test/'\n",
        "num_workers = 2\n",
        "batch_size = 1\n",
        "Learned_model= model\n",
        "test_set = TestDataset(data_path)\n",
        "test_data_loader = DataLoader(dataset=test_set, num_workers=num_workers,batch_size=batch_size, shuffle=False)\n",
        "Learned_model.eval()\n",
        "for iteration, sample in enumerate(test_data_loader):\n",
        "    img,img_pth = sample\n",
        "    #print(img.shape)\n",
        "    print(img.size())\n",
        "    img = img.unsqueeze(1)\n",
        "    print(img.size())\n",
        "    \n",
        "    #img = x(img)\n",
        "\n",
        "    img = img.to(device)\n",
        "    out = Learned_model(img)\n",
        "    out_np = torch.max(out,1).indices.cpu().detach().numpy()\n",
        "    image_np = img.cpu().detach().numpy()\n",
        "    print(img_pth[0])\n",
        "   # print(re.split(r'\\/',img_pth[0]))\n",
        "    img_pth = re.split(r'\\/',img_pth[0])\n",
        "    print(img_pth)\n",
        "    path = os.path.join('',img_pth[-1])\n",
        "    print(path)\n",
        "    cv2.imwrite(np.float32(path),out*85)   #\n",
        "    for i in range(1):\n",
        "            show_image_mask(image_np[i,0], out_np[i], cmap='gray')\n",
        "            plt.pause(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 96, 96])\n",
            "torch.Size([1, 1, 96, 96])\n",
            "/content/drive/MyDrive/data/test/image/cmr161.png\n",
            "['', 'content', 'drive', 'MyDrive', 'data', 'test', 'image', 'cmr161.png']\n",
            "cmr161.png\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-2b9a4bc6cbe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_pth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m85\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mshow_image_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'cmr161.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsycVbIuUov3"
      },
      "source": [
        "## 3 Evaluation\n",
        "\n",
        "As we will automatically evaluate your predicted test makes on Kaggle, in this section we expect you to learn:\n",
        "* what is the Dice score used on Kaggle to measure your models performance\n",
        "* how to submit your predicted masks to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NicQyj47jsD1"
      },
      "source": [
        "### 3.1 Dice Score\n",
        "\n",
        "To evaluate the quality of the predicted masks, the Dice score is adopted. Dice score on two masks A and B is defined as the intersection ratio between the overlap area and the average area of two masks. A higher Dice suggests a better registration.\n",
        "\n",
        "$Dice (A, B)= \\frac{2|A \\cap B|}{|A| + |B|} $\n",
        "\n",
        "However, in our coursework, we have three labels in each mask, we will compute the Dice score for each label and then average the three of them as the final score. Below we have given you `categorical_dice` for free so you can test your results before submission to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzOY4GROqW_V"
      },
      "source": [
        "def categorical_dice(mask1, mask2, label_class=1):\n",
        "    \"\"\"\n",
        "    Dice score of a specified class between two volumes of label masks.\n",
        "    (classes are encoded but by label class number not one-hot )\n",
        "    Note: stacks of 2D slices are considered volumes.\n",
        "\n",
        "    Args:\n",
        "        mask1: N label masks, numpy array shaped (H, W, N)\n",
        "        mask2: N label masks, numpy array shaped (H, W, N)\n",
        "        label_class: the class over which to calculate dice scores\n",
        "\n",
        "    Returns:\n",
        "        volume_dice\n",
        "    \"\"\"\n",
        "    mask1_pos = (mask1 == label_class).astype(np.float32)\n",
        "    mask2_pos = (mask2 == label_class).astype(np.float32)\n",
        "    dice = 2 * np.sum(mask1_pos * mask2_pos) / (np.sum(mask1_pos) + np.sum(mask2_pos))\n",
        "    return dice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZcsrwmVjy5k"
      },
      "source": [
        "### 3.2 Submission\n",
        "\n",
        "Kaggle requires your submission to be in a specific CSV format. To help ensure your submissions are in the correct format, we have provided some helper functions to do this for you. For those interested, the png images are run-length encoded and saved in a CSV to the specifications required by our competition.\n",
        "\n",
        "It is sufficient to use this helper function. To do so, save your 80 predicted masks into a directory. ONLY the 80 predicted masks should be in this directory. Call the submission_converter function with the first argument as the directory containing your masks, and the second the directory in which you wish to save your submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHDVbgu0qW_V"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "def rle_encoding(x):\n",
        "    '''\n",
        "    *** Credit to https://www.kaggle.com/rakhlin/fast-run-length-encoding-python ***\n",
        "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
        "    Returns run length as list\n",
        "    '''\n",
        "    dots = np.where(x.T.flatten() == 1)[0]\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return run_lengths\n",
        "\n",
        "\n",
        "def submission_converter(mask_directory, path_to_save):\n",
        "    writer = open(os.path.join(path_to_save, \"submission.csv\"), 'w')\n",
        "    writer.write('id,encoding\\n')\n",
        "\n",
        "    files = os.listdir(mask_directory)\n",
        "\n",
        "    for file in files:\n",
        "        name = file[:-4]\n",
        "        mask = cv2.imread(os.path.join(mask_directory, file), cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "        mask1 = (mask == 1)\n",
        "        mask2 = (mask == 2)\n",
        "        mask3 = (mask == 3)\n",
        "\n",
        "        encoded_mask1 = rle_encoding(mask1)\n",
        "        encoded_mask1 = ' '.join(str(e) for e in encoded_mask1)\n",
        "        encoded_mask2 = rle_encoding(mask2)\n",
        "        encoded_mask2 = ' '.join(str(e) for e in encoded_mask2)\n",
        "        encoded_mask3 = rle_encoding(mask3)\n",
        "        encoded_mask3 = ' '.join(str(e) for e in encoded_mask3)\n",
        "\n",
        "        writer.write(name + '1,' + encoded_mask1 + \"\\n\")\n",
        "        writer.write(name + '2,' + encoded_mask2 + \"\\n\")\n",
        "        writer.write(name + '3,' + encoded_mask3 + \"\\n\")\n",
        "\n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bOn_j_FqW_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46cc8a32-f176-469e-b63b-e86ecaf319ff"
      },
      "source": [
        "well = \"Well Boy\"\n",
        "print(well)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Well Boy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g_jvJUntGwh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}